{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Options in Optim.jl\n",
    "This notebook shows how to configure the default options in Optim.jl. Numerical optimization can sometimes require a bit of experimentation to get right in a given context, and this requires the ability to change various details, but also study how the iterations proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Method options\n",
    "Many of the methods in Optim.jl are developped as, and meant to be, general purpose non-linear optimization procedures, if there ever was such a thing. However, no matter what algorithm you use, there are choices of constants, line searches, and more. These things can have significant influence on the success of locating a minimum, so it's important to know how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Line search choice\n",
    "Many methods in Optim.jl are build around determining a *search direction*, $s$. This direction is not the final step-size however. To guarantee convergence to a critical point, we use line search algorithms to find a step size $\\alpha$, such that the final update\n",
    "$$\n",
    "    x_{k+1}=x_k+\\alpha\\cdot s\n",
    "$$\n",
    "To illustrate, let's find a minimizer to the objective function we will call \"Large Polynomial\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h! (generic function with 2 methods)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f(x)\n",
    "    return (x[1] + 10.0 * x[2])^2 + 5.0 * (x[3] - x[4])^2 +\n",
    "            (x[2] - 2.0 * x[3])^4 + 10.0 * (x[1] - x[4])^4\n",
    "end\n",
    "\n",
    "function g!(storage, x)\n",
    "    storage[1] = 2.0 * (x[1] + 10.0 * x[2]) + 40.0 * (x[1] - x[4])^3\n",
    "    storage[2] = 20.0 * (x[1] + 10.0 * x[2]) + 4.0 * (x[2] - 2.0 * x[3])^3\n",
    "    storage[3] = 10.0 * (x[3] - x[4]) - 8.0 * (x[2] - 2.0 * x[3])^3\n",
    "    storage[4] = -10.0 * (x[3] - x[4]) - 40.0 * (x[1] - x[4])^3\n",
    "end\n",
    "\n",
    "function h!(storage, x)\n",
    "    storage[1, 1] = 2.0 + 120.0 * (x[1] - x[4])^2\n",
    "    storage[1, 2] = 20.0\n",
    "    storage[1, 3] = 0.0\n",
    "    storage[1, 4] = -120.0 * (x[1] - x[4])^2\n",
    "    storage[2, 1] = 20.0\n",
    "    storage[2, 2] = 200.0 + 12.0 * (x[2] - 2.0 * x[3])^2\n",
    "    storage[2, 3] = -24.0 * (x[2] - 2.0 * x[3])^2\n",
    "    storage[2, 4] = 0.0\n",
    "    storage[3, 1] = 0.0\n",
    "    storage[3, 2] = -24.0 * (x[2] - 2.0 * x[3])^2\n",
    "    storage[3, 3] = 10.0 + 48.0 * (x[2] - 2.0 * x[3])^2\n",
    "    storage[3, 4] = -10.0\n",
    "    storage[4, 1] = -120.0 * (x[1] - x[4])^2\n",
    "    storage[4, 2] = 0.0\n",
    "    storage[4, 3] = -10.0\n",
    "    storage[4, 4] = 10.0 + 120.0 * (x[1] - x[4])^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to optimize this function from a starting point of $x=[3, -1, 0, 1]$, and the result should be the zero vector. We explicitly choose the line search by Hager and Zhang developped for the very Conjugate Gradient implementation we have in Optim.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Conjugate Gradient\n",
       " * Starting Point: [3.0,-1.0,0.0,1.0]\n",
       " * Minimizer: [0.003075918767500598,-0.00030759239566266136, ...]\n",
       " * Minimum: 1.864396e-10\n",
       " * Iterations: 1000\n",
       " * Convergence: false\n",
       "   * |x - x'| < 1.0e-32: false \n",
       "     |x - x'| = 3.70e-06 \n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
       "     |f(x) - f(x')| / |f(x)| = 4.83e-03 \n",
       "   * |g(x)| < 1.0e-08: false \n",
       "     |g(x)| = 2.83e-06 \n",
       "   * stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: true\n",
       " * Objective Calls: 2002\n",
       " * Gradient Calls: 1177"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim\n",
    "optimize(f, g!, h!, [3.0, -1.0, 0.0, 1.0], ConjugateGradient(linesearch=LineSearches.HagerZhang()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that the convergence information tells us that we are not at a local minimizer. The procedure stopped because we reached the default value of 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Conjugate Gradient\n",
       " * Starting Point: [3.0,-1.0,0.0,1.0]\n",
       " * Minimizer: [-0.0006377700262345379,6.377700370473088e-5, ...]\n",
       " * Minimum: 3.515308e-13\n",
       " * Iterations: 469\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-32: false \n",
       "     |x - x'| = 3.39e-09 \n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
       "     |f(x) - f(x')| / |f(x)| = 1.73e-05 \n",
       "   * |g(x)| < 1.0e-08: true \n",
       "     |g(x)| = 9.96e-09 \n",
       "   * stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 948\n",
       " * Gradient Calls: 571"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, h!, [3.0, -1.0, 0.0, 1.0], ConjugateGradient(linesearch=LineSearches.MoreThuente()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a different line search algorithm by More and Thuente, we get much better performance. Let us see how Newton's method that uses the Hessian information does with the Hager and Zhang line search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method\n",
       " * Starting Point: [3.0,-1.0,0.0,1.0]\n",
       " * Minimizer: [0.00018829366513309454,-1.8829395155003336e-5, ...]\n",
       " * Minimum: 6.297554e-15\n",
       " * Iterations: 10\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-32: false \n",
       "     |x - x'| = 9.24e-05 \n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
       "     |f(x) - f(x')| / |f(x)| = NaN \n",
       "   * |g(x)| < 1.0e-08: true \n",
       "     |g(x)| = 5.73e-09 \n",
       "   * stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 31\n",
       " * Gradient Calls: 31\n",
       " * Hessian Calls: 10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, h!, [3.0, -1.0, 0.0, 1.0], Newton(linesearch=LineSearches.HagerZhang()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it does much better, will we see a difference once more when switching to More and Thuente's line search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method\n",
       " * Starting Point: [3.0,-1.0,0.0,1.0]\n",
       " * Minimizer: [0.0007160206186411141,-7.160206186411139e-5, ...]\n",
       " * Minimum: 1.316816e-12\n",
       " * Iterations: 20\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-32: false \n",
       "     |x - x'| = 3.58e-04 \n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: false\n",
       "     |f(x) - f(x')| / |f(x)| = NaN \n",
       "   * |g(x)| < 1.0e-08: true \n",
       "     |g(x)| = 8.70e-09 \n",
       "   * stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 21\n",
       " * Gradient Calls: 21\n",
       " * Hessian Calls: 20"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(f, g!, h!, [3.0, -1.0, 0.0, 1.0], Newton(linesearch=LineSearches.MoreThuente()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 General options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some options are algorithm specific, some are sufficiently general, that they're included in the `Optim.Options` type. Things in this category are: number of iterations, convergence criterions, etc. Below, we see the the different keywords and their default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```julia\n",
    "Options(;\n",
    "        x_tol::Real = 1e-32,\n",
    "        f_tol::Real = 1e-32,\n",
    "        g_tol::Real = 1e-8,\n",
    "        f_calls_limit::Int = 0,\n",
    "        g_calls_limit::Int = 0,\n",
    "        h_calls_limit::Int = 0,\n",
    "        time_limit = NaN,\n",
    "        allow_f_increases::Bool = false,\n",
    "        iterations::Integer = 1_000,\n",
    "        store_trace::Bool = false,\n",
    "        show_trace::Bool = false,\n",
    "        extended_trace::Bool = false,\n",
    "        show_every::Integer = 1,\n",
    "        callback = nothing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three define convergence tolerances, the next three define soft limits on the number of calls to the objective function, the gradient and the hessian, then there's a time limit (in seconds), an option to allow the objective to increase while iterating, the number of iterations we allow, some trace related options and lastly you can pass a callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
